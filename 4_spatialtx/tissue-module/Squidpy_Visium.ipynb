{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Visium fluorescence data\n",
    "================================\n",
    "\n",
    "This tutorial shows how to apply Squidpy image analysis features for the\n",
    "analysis of Visium data.\n",
    "\n",
    "For a tutorial using Visium data that includes the graph analysis\n",
    "functions, have a look at\n",
    "`sphx_glr_auto_tutorials_tutorial_visium_hne.py`. The dataset used here\n",
    "consists of a Visium slide of a coronal section of the mouse brain. The\n",
    "original dataset is publicly available at the 10x genomics [dataset\n",
    "portal](https://support.10xgenomics.com/spatial-gene-expression/datasets)\n",
    ". Here, we provide a pre-processed dataset, with pre-annotated clusters,\n",
    "in `anndata.AnnData` format and the tissue image in\n",
    "`squidpy.im.ImageContainer` format.\n",
    "\n",
    "A couple of notes on pre-processing:\n",
    "\n",
    "> -   The pre-processing pipeline is the same as the one shown in the\n",
    ">     original [Scanpy\n",
    ">     tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html)\n",
    ">     .\n",
    "> -   The cluster annotation was performed using several resources, such\n",
    ">     as the [Allen Brain\n",
    ">     Atlas](https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas)\n",
    ">     , the [Mouse Brain gene expression atlas](http://mousebrain.org/)\n",
    ">     from the Linnarson lab and this recent pre-print `linnarson2020`.\n",
    "\n",
    "::: {.seealso}\n",
    "See `sphx_glr_auto_tutorials_tutorial_visium_hne.py` for additional\n",
    "analysis examples.\n",
    ":::\n",
    "\n",
    "Import packages & data\n",
    "----------------------\n",
    "\n",
    "To run the notebook locally, create a conda environment as *conda env\n",
    "create -f environment.yml* using this\n",
    "[environment.yml](https://github.com/scverse/squidpy_notebooks/blob/main/environment.yml).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import squidpy as sq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sc.logging.print_header()\n",
    "print(f\"squidpy=={sq.__version__}\")\n",
    "\n",
    "# load the pre-processed dataset\n",
    "img = sq.datasets.visium_fluo_image_crop()\n",
    "adata = sq.datasets.visium_fluo_adata_crop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let\\'s visualize the cluster annotation in the spatial context\n",
    "with `scanpy.pl.spatial`.\n",
    "\n",
    "As you can see, this dataset is a smaller crop of the whole brain\n",
    "section. We provide this crop to make the execution time of this\n",
    "tutorial a bit shorter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sc.pl.spatial(adata, color=\"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fluorescence image provided with this dataset has three channels:\n",
    "*DAPI* (specific to DNA), *anti-NEUN* (specific to neurons), *anti-GFAP*\n",
    "(specific to Glial cells). We can directly visualize the channels with\n",
    "the method `squidpy.im.ImageContainer.show`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img.show(channelwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visium datasets contain high-resolution images of the tissue. Using the\n",
    "function `squidpy.im.calculate_image_features` you can calculate image\n",
    "features for each Visium spot and create a `obs x features` matrix in\n",
    "`adata` that can then be analyzed together with the `obs x gene` gene\n",
    "expression matrix.\n",
    "\n",
    "By extracting image features we are aiming to get both similar and\n",
    "complementary information to the gene expression values. Similar\n",
    "information is for example present in the case of a tissue with two\n",
    "different cell types whose morphology is different. Such cell type\n",
    "information is then contained in both the gene expression values and the\n",
    "tissue image features. Complementary or additional information is\n",
    "present in the fact that we can use a nucleus segmentation to count\n",
    "cells and add features summarizing the immediate spatial neighborhood of\n",
    "a spot.\n",
    "\n",
    "Squidpy contains several feature extractors and a flexible pipeline of\n",
    "calculating features of different scales and sizes. There are several\n",
    "detailed examples of how to use `squidpy.im.calculate_image_features`.\n",
    "`sphx_glr_auto_examples_image_compute_features.py` provides a good\n",
    "starting point for learning more.\n",
    "\n",
    "Here, we will extract [summary]{.title-ref}, [histogram]{.title-ref},\n",
    "[segmentation]{.title-ref}, and [texture]{.title-ref} features. To\n",
    "provide more context and allow the calculation of multi-scale features,\n",
    "we will additionally calculate [summary]{.title-ref} and\n",
    "[histogram]{.title-ref} features at different crop sizes and scales.\n",
    "\n",
    "Image segmentation\n",
    "==================\n",
    "\n",
    "To calculate [segmentation]{.title-ref} features, we first need to\n",
    "segment the tissue image using `squidpy.im.segment`. But even before\n",
    "that, it\\'s best practice to pre-process the image by e.g. smoothing it\n",
    "using in `squidpy.im.process`. We will then use the *DAPI* channel of\n",
    "the fluorescence image (`channel_id s= 0`). Please refer to\n",
    "`sphx_glr_auto_examples_image_compute_segment_fluo.py` for more details\n",
    "on how to calculate a segmented image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sq.im.process(\n",
    "    img=img,\n",
    "    layer=\"image\",\n",
    "    method=\"smooth\",\n",
    ")\n",
    "\n",
    "sq.im.segment(img=img, layer=\"image_smooth\", method=\"watershed\", channel=0, chunks=1000)\n",
    "\n",
    "# plot the resulting segmentation\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "img_crop = img.crop_corner(2000, 2000, size=500)\n",
    "img_crop.show(layer=\"image\", channel=0, ax=ax[0])\n",
    "img_crop.show(\n",
    "    layer=\"segmented_watershed\",\n",
    "    channel=0,\n",
    "    ax=ax[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `squidpy.im.segment` is saved in\n",
    "`img['segmented_watershed']` by default. It is a label image where each\n",
    "segmented object is annotated with a different integer number.\n",
    "\n",
    "Segmentation features\n",
    "=====================\n",
    "\n",
    "We can now use the segmentation to calculate segmentation features.\n",
    "These include morphological features of the segmented objects and\n",
    "channel-wise image intensities beneath the segmentation mask. In\n",
    "particular, we can count the segmented objects within each Visium spot\n",
    "to get an approximation of the number of cells. In addition, we can\n",
    "calculate the mean intensity of each fluorescence channel within the\n",
    "segmented objects. Depending on the fluorescence channels, this can give\n",
    "us e.g., an estimation of the cell type. For more details on how the\n",
    "segmentation features, you can have a look at the docs of\n",
    "`squidpy.im.calculate_image_features` or the example at\n",
    "`sphx_glr_auto_examples_image_compute_segmentation_features.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define image layer to use for segmentation\n",
    "features_kwargs = {\"segmentation\": {\"label_layer\": \"segmented_watershed\"}}\n",
    "# calculate segmentation features\n",
    "sq.im.calculate_image_features(\n",
    "    adata,\n",
    "    img,\n",
    "    features=\"segmentation\",\n",
    "    layer=\"image\",\n",
    "    key_added=\"features_segmentation\",\n",
    "    n_jobs=1,\n",
    "    features_kwargs=features_kwargs,\n",
    ")\n",
    "# plot results and compare with gene-space clustering\n",
    "sc.pl.spatial(\n",
    "    sq.pl.extract(adata, \"features_segmentation\"),\n",
    "    color=[\n",
    "        \"segmentation_label\",\n",
    "        \"cluster\",\n",
    "        \"segmentation_ch-0_mean_intensity_mean\",\n",
    "        \"segmentation_ch-1_mean_intensity_mean\",\n",
    "    ],\n",
    "    frameon=False,\n",
    "    ncols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we made use of `squidpy.pl.extract`, a method to extract all\n",
    "features in a given [adata.obsm\\[\\'{key}\\'\\]]{.title-ref} and\n",
    "temporarily save them to `anndata.AnnData.obs`. Such method is\n",
    "particularly useful for plotting purpose, as shown above.\n",
    "\n",
    "The number of cells per Visium spot provides an interesting view of the\n",
    "data that can enhance the characterization of gene-space clusters. We\n",
    "can see that the cell-rich pyramidal layer of the Hippocampus has more\n",
    "cells than the surrounding areas (upper left). This fine-grained view of\n",
    "the Hippocampus is not visible in the gene clusters where the\n",
    "Hippocampus is one cluster only.\n",
    "\n",
    "The per-channel intensities plotted in the second row show us that the\n",
    "areas labeled with *Cortex\\_1* and *Cortex\\_3* have a higher intensity\n",
    "of channel 1, *anti-NEUN* (lower left). This means that these areas have\n",
    "more neurons that the remaining areas in this crop. In addition, cluster\n",
    "*Fiber\\_tracts* and *lateral ventricles* seems to be enriched with\n",
    "*Glial cells*, seen by the larger mean intensities of channel 2,\n",
    "*anti-GFAP*, in these areas (lower right).\n",
    "\n",
    "Extract and cluster features\n",
    "============================\n",
    "\n",
    "Now we will calculate summary, histogram, and texture features. These\n",
    "features provide a useful compressed summary of the tissue image. For\n",
    "more information on these features, refer to:\n",
    "\n",
    "> -   `sphx_glr_auto_examples_image_compute_summary_features.py`.\n",
    "> -   `sphx_glr_auto_examples_image_compute_histogram_features.py`.\n",
    "> -   `sphx_glr_auto_examples_image_compute_texture_features.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define different feature calculation combinations\n",
    "params = {\n",
    "    # all features, corresponding only to tissue underneath spot\n",
    "    \"features_orig\": {\n",
    "        \"features\": [\"summary\", \"texture\", \"histogram\"],\n",
    "        \"scale\": 1.0,\n",
    "        \"mask_circle\": True,\n",
    "    },\n",
    "    # summary and histogram features with a bit more context, original resolution\n",
    "    \"features_context\": {\"features\": [\"summary\", \"histogram\"], \"scale\": 1.0},\n",
    "    # summary and histogram features with more context and at lower resolution\n",
    "    \"features_lowres\": {\"features\": [\"summary\", \"histogram\"], \"scale\": 0.25},\n",
    "}\n",
    "\n",
    "for feature_name, cur_params in params.items():\n",
    "    # features will be saved in `adata.obsm[feature_name]`\n",
    "    sq.im.calculate_image_features(adata, img, layer=\"image\", key_added=feature_name, n_jobs=1, **cur_params)\n",
    "\n",
    "# combine features in one dataframe\n",
    "adata.obsm[\"features\"] = pd.concat([adata.obsm[f] for f in params.keys()], axis=\"columns\")\n",
    "\n",
    "# make sure that we have no duplicated feature names in the combined table\n",
    "adata.obsm[\"features\"].columns = ad.utils.make_index_unique(adata.obsm[\"features\"].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the extracted image features to compute a new cluster\n",
    "annotation. This could be useful to gain insights in similarities across\n",
    "spots based on image morphology.\n",
    "\n",
    "For this, we first define a helper function to cluster features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cluster_features(features: pd.DataFrame, like=None):\n",
    "    \"\"\"\n",
    "    Calculate leiden clustering of features.\n",
    "\n",
    "    Specify filter of features using `like`.\n",
    "    \"\"\"\n",
    "    # filter features\n",
    "    if like is not None:\n",
    "        features = features.filter(like=like)\n",
    "    # create temporary adata to calculate the clustering\n",
    "    adata = ad.AnnData(features)\n",
    "    # important - feature values are not scaled, so need to scale them before PCA\n",
    "    sc.pp.scale(adata)\n",
    "    # calculate leiden clustering\n",
    "    sc.pp.pca(adata, n_comps=min(10, features.shape[1] - 1))\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.leiden(adata)\n",
    "\n",
    "    return adata.obs[\"leiden\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculate feature clusters using different features and compare\n",
    "them to gene clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "adata.obs[\"features_summary_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"summary\")\n",
    "adata.obs[\"features_histogram_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"histogram\")\n",
    "adata.obs[\"features_texture_cluster\"] = cluster_features(adata.obsm[\"features\"], like=\"texture\")\n",
    "\n",
    "sc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n",
    "sc.pl.spatial(\n",
    "    adata,\n",
    "    color=[\n",
    "        \"features_summary_cluster\",\n",
    "        \"features_histogram_cluster\",\n",
    "        \"features_texture_cluster\",\n",
    "        \"cluster\",\n",
    "    ],\n",
    "    ncols=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the gene-space clusters (bottom middle), the feature space clusters\n",
    "are also spatially coherent.\n",
    "\n",
    "The feature clusters of the different feature extractors are quite\n",
    "diverse, but all of them reflect the structure of the Hippocampus by\n",
    "assigning different clusters to the different structural areas. This is\n",
    "a higher level of detail than the gene-space clustering provides with\n",
    "only one cluster for the Hippocampus.\n",
    "\n",
    "The feature clusters also show the layered structure of the cortex, but\n",
    "again subdividing it in more clusters than the gene-space clustering. It\n",
    "might be possible to re-cluster the gene expression counts with a higher\n",
    "resolution to also get more fine-grained clusters, but nevertheless the\n",
    "image features seem to provide additional supporting information to the\n",
    "gene-space clusters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
